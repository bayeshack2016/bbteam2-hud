{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pickle\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "punc = set(string.punctuation)\n",
    "for sw in punc:\n",
    "    stops.add(sw)\n",
    "\n",
    "dir_ = r\"C:\\Users\\Broccoli\\Documents\\Bayes Hack\"\n",
    "hp2014 = pickle.load(open(dir_ + r\"\\housing_pipeline_2014_comments.pkl\", \"rb\"))\n",
    "hi2014 = pickle.load(open(dir_ + r\"\\housing_inventory_2014_comments.pkl\", \"rb\"))\n",
    "hi2013 = pickle.load(open(dir_ + r\"\\housing_inventory_2013_comments.pkl\", \"rb\"))\n",
    "hi2012 = pickle.load(open(dir_ + r\"\\housing_inventory_2012_comments.pkl\", \"rb\"))\n",
    "\n",
    "comments = {}\n",
    "for d in [hp2014,hi2014,hi2013,hi2012]:\n",
    "    for k,v in d.items():\n",
    "        comments[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def iter_comments(comments=comments):\n",
    "    for prj in comments:\n",
    "        for article in comments[prj]:\n",
    "            for c in article['comments']:\n",
    "                yield ' '.join([''.join(ch for ch in w if ch not in punc).lower() for w in c.split(' ')])\n",
    "\n",
    "def iter_articles(comments=comments):         \n",
    "    for prj in comments:\n",
    "        for article in comments[prj]:\n",
    "            yield ' '.join([' '.join([''.join(ch for ch in w if ch not in punc).lower() for w in c.split(' ')]) for c in article['comments']])\n",
    "        \n",
    "com2prj = []\n",
    "for prj in comments:\n",
    "    for article in comments[prj]:\n",
    "        [com2prj.append(prj) for c in range(0,len(article['comments']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentiment_pred(sentence):\n",
    "    from nltk.corpus import opinion_lexicon\n",
    "    from nltk.tokenize import treebank\n",
    "#    with open(dir_+r'\\negative.txt', 'r') as f:\n",
    "#        negative = set(f.read().split('\\n'))\n",
    "    negative = opinion_lexicon.negative()\n",
    "    tokenizer = treebank.TreebankWordTokenizer()\n",
    "    pos_words = 0\n",
    "    neg_words = 0\n",
    "    tokenized_sent = [word.lower() for word in tokenizer.tokenize(sentence) if word not in stops]\n",
    "    x = list(range(len(tokenized_sent))) # x axis for the plot\n",
    "    y = []\n",
    "    for word in tokenized_sent:\n",
    "        if word in opinion_lexicon.positive():\n",
    "            pos_words += 1\n",
    "            y.append(1) # positive\n",
    "        elif any(n in word for n in negative):\n",
    "            neg_words += 1\n",
    "            y.append(-1) # negative\n",
    "        else:\n",
    "            y.append(0) # neutral\n",
    "    if pos_words > neg_words:\n",
    "        return 1\n",
    "    elif pos_words < neg_words:\n",
    "        return -1\n",
    "    elif pos_words == neg_words:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comments = [[word for word in comment.split() if word not in stops] for comment in iter_articles()]\n",
    "id2word = corpora.Dictionary(comments)\n",
    "id2word.filter_extremes(no_below=1, no_above=.90)\n",
    "comment_corpus = [id2word.doc2bow(c) for c in comments]\n",
    "\n",
    "lda = gensim.models.ldamodel.LdaModel(comment_corpus, id2word=id2word, num_topics=15, passes=10)\n",
    "#lsi = gensim.models.LsiModel(comment_corpus, id2word=id2word, num_topics=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "for t in lda.print_topics(num_topics=15):\n",
    "    print(re.sub(r'[0-9\\*\\+\\.]', '', t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 71459)\n",
      "(30905, 10)\n",
      "0\n",
      "0\n",
      "building , parking , people , city , sf , housing , don , new , think , market , lot , good , street , units , really , build , san , buildings , project , area , francisco , san francisco , going , know , want\n",
      "1\n",
      "parking , spaces , cars , parking spaces , street , lot , parking lot , car , street parking , space , garage , parking space , transit , spots , bike , building , parking garage , parking spots , congestion , units , retail , ratio , bike parking , unit , spot\n",
      "2\n",
      "parking , san , san francisco , francisco , cars , spaces , parking spaces , car , city , street parking , parking lot , street , county , transit , county san , garage , bike , city san , parking space , congestion , spots , parking garage , parking spots , property county , people\n",
      "3\n",
      "building , design , new , buildings , francisco , san francisco , san , new building , lot , story , nice , height , planning , proposed , story building , taller , space , love , ugly , corner , tall , street , parking , windows , facade\n",
      "4\n",
      "housing , units , affordable , building , market , affordable housing , build , city , rate , market rate , sf , new , rent , bmr , built , cost , control , rent control , unit , luxury , costs , housing units , needs , public housing , income\n",
      "5\n",
      "good , nice , design , great , yes , sf , project , lol , futurist , better , neighborhood , anon , location , parking , news , luck , area , love , good luck , development , planning , built , exactly , space , good news\n",
      "6\n",
      "sf , people , new , city , housing , build , design , nice , live , good , building , buildings , transit , cars , want , area , car , bay , density , public , built , neighborhoods , great , futurist , love\n",
      "7\n",
      "yes , sf , building , people , lot , years , yes yes , right , car , real , house , live , lol , ago , san , years ago , anon , estate , ugly , real estate , san francisco , francisco , yes agree , market , parking lot\n",
      "8\n",
      "good , don , yes , housing , lol , know , build , affordable , building , want , affordable housing , anon , agree , does , project , units , think , don know , thanks , don think , people , futurist , thing , luck , good luck\n",
      "9\n",
      "lol , build , lot , new , love , going , say , buildings , right , futurist , house , development , great , brahma , agree , area , housing , people , real , neighborhood , sure , height , better , agreed , taller\n"
     ]
    }
   ],
   "source": [
    "reducedDim = 10\n",
    "\n",
    "def iter4lda(ic=iter_comments()):\n",
    "    for comment in ic:\n",
    "        yield ' '.join([w for w in comment.split(' ') if all(sw not in w for sw in ['like','look','just'])])\n",
    "\n",
    "comment2address = []\n",
    "tfidf_vectorizer = text.TfidfVectorizer(min_df=2, stop_words='english', strip_accents = 'unicode', lowercase=True, \n",
    "                                        ngram_range=(1,2), norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(iter4lda())\n",
    "words_tfidf = np.array(tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "tsvd_tfidf = TruncatedSVD(reducedDim)\n",
    "tsvd_tfidf.fit(tfidf_matrix)\n",
    "V_tf = tsvd_tfidf.components_\n",
    "U_tf = tsvd_tfidf.transform(tfidf_matrix)\n",
    "print(V_tf.shape)\n",
    "print(U_tf.shape)\n",
    "print(len(comment2address))\n",
    "\n",
    "topic_words={}\n",
    "for i in range(reducedDim):\n",
    "    row = tsvd_tfidf.components_[i,:]\n",
    "    argsorted = np.argsort(row)[::-1][:25]\n",
    "    scores = row[argsorted]\n",
    "    top_words = words_tfidf[argsorted]\n",
    "    topic_words[i]=' , '.join(top_words)\n",
    "for i in topic_words:\n",
    "    print(i)\n",
    "    print(topic_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_vectorizer = text.CountVectorizer(analyzer='word', ngram_range=(1,2), min_df = 0, stop_words = 'english', lowercase=True)\n",
    "count_matrix = count_vectorizer.fit_transform(iter4lda())\n",
    "words_count = np.array(count_vectorizer.get_feature_names())\n",
    "\n",
    "tsvd_count = TruncatedSVD(reducedDim)\n",
    "tsvd_count.fit(count_matrix)\n",
    "V_tc = tsvd_count.components_\n",
    "U_tc = tsvd_count.transform(count_matrix)\n",
    "print(V_tc.shape)\n",
    "print(U_tc.shape)\n",
    "print(len(comment2address))\n",
    "\n",
    "topic_words_count={}\n",
    "for i in range(reducedDim):\n",
    "    row = tsvd_count.components_[i,:]\n",
    "    argsorted = np.argsort(row)[::-1][:25]\n",
    "    scores = row[argsorted]\n",
    "    top_words = words_count[argsorted]\n",
    "    topic_words_count[i]=' , '.join(top_words)\n",
    "for i in topic_words_count:\n",
    "    print(i)\n",
    "    print(topic_words_count[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.sample([c for c in iter_comments()], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "address2topic = defaultdict(partial(np.ndarray, reducedDim))\n",
    "\n",
    "row = 0\n",
    "for address in comment2address:\n",
    "    address2topic[address] += U_tf[row]\n",
    "    row += 1\n",
    "\n",
    "topic2address = {}\n",
    "for i in range(1,10):\n",
    "    topic2address[i] = [a for a in address2topic if address2topic[a][i] == max(address2topic[a][1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0,9):\n",
    "    if address2topic['1078 hampshire st'][i] == max(address2topic['1078 hampshire st']):\n",
    "        print(topic_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prjsent = defaultdict(list)\n",
    "\n",
    "for prj in random.sample(list(comments),25):\n",
    "    for article in comments[prj]:\n",
    "            sentiment = Counter()\n",
    "            for com in article['comments']:\n",
    "                sentiment.update([str(sentiment_pred(' '.join([''.join(ch for ch in w if ch not in punc).lower() for w in com.split(' ')])))])\n",
    "            prjsent[prj].append(sentiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
